# -*- coding: utf-8 -*-
"""Weather_Prediction_Capstone_Project_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SoS4zPv511rhH9mhkmrHgHZjzoPCwG6W

## Capstone Project: Prediksi Cuaca Sederhana Berbasis Data Historis dengan Machine Learning dan Visualisasi Web

**ID Team:** CC25-CR363

### ðŸ“Š Deskripsi Dataset

**Sumber**: [BMKG (Badan Meteorologi, Klimatologi, dan Geofisika)](https://dataonline.bmkg.go.id/)

**Periode Data**: Tahun **2013 hingga 2024**  
**Provinsi**: DKI Jakarta  
**Kota**: Jakarta Pusat  
**Stasiun**: Stasiun Meteorologi Kemayoran

Dataset ini berisi data pengamatan cuaca harian yang dikumpulkan oleh BMKG dari Stasiun Meteorologi Kemayoran. Data mencakup parameter-parameter penting seperti:

- Suhu minimum, maksimum, dan rata-rata (Â°C)
- Kelembaban rata-rata (%)
- Curah hujan harian (mm)
- Lama penyinaran matahari (jam)
- Kecepatan dan arah angin maksimum serta rata-rata

### ðŸŽ¯ Tujuan Proyek

Membangun model machine learning untuk memprediksi **kondisi cuaca harian** di Jakarta Pusat berdasarkan data historis. Label cuaca diklasifikasikan menjadi:

## 1. Import Library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score
from sklearn.model_selection import GridSearchCV

from imblearn.over_sampling import SMOTE

from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.neural_network import MLPClassifier

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical

import joblib

"""## 2. Data Preparation"""

# Load dataset dari GitHub
url = 'https://raw.githubusercontent.com/melshitaardia/Weather-Prediction/main/datacuaca_BMKG.csv'
df = pd.read_csv(url)

# Tampilkan 5 data teratas
df.head()

"""## 3. Data Preprocessing"""

# Buat label cuaca
def generate_label(precip):
    if precip <= 0.2:
        return "cerah"
    elif precip <= 1.0:
        return "mendung"
    else:
        return "hujan"

df['cuaca'] = df['precip_mm'].apply(generate_label)

df_cleaned = df.drop(columns=['tanggal', 'source_file', 'wind_dir_common_deg'])
label_encoder = LabelEncoder()
df_cleaned['cuaca_encoded'] = label_encoder.fit_transform(df_cleaned['cuaca'])

X = df_cleaned.drop(columns=['cuaca', 'cuaca_encoded'])
y = df_cleaned['cuaca_encoded']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

"""### Data splitting"""

X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, stratify=y, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

"""### Balancing (SMOTE)"""

smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

"""## 5. Modeling

### Random Forest Classifier
"""

rf_clf = RandomForestClassifier(random_state=42)
rf_clf.fit(X_train_smote, y_train_smote)

rf_pred = rf_clf.predict(X_test)

"""### KNN"""

knn_clf = KNeighborsClassifier(n_neighbors=5)
knn_clf.fit(X_train_smote, y_train_smote)

knn_pred = knn_clf.predict(X_test)

"""### XGBoost Classifier"""

xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
xgb_clf.fit(X_train_smote, y_train_smote)

xgb_pred = xgb_clf.predict(X_test)

"""### MLPClassifier"""

mlp_clf = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=200, alpha=0.0001,
                        solver='adam', random_state=42)
mlp_clf.fit(X_train_smote, y_train_smote)

mlp_pred = mlp_clf.predict(X_test)

"""### TensorFlow MLP (Deep Learning)"""

model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train_smote.shape[1],)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(3, activation='softmax')  # 3 kelas
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

history = model.fit(
    X_train_smote, y_train_smote,
    validation_data=(X_val, y_val),
    epochs=100,
    batch_size=32,
    callbacks=[early_stop],
    verbose=1
)

# Prediksi TensorFlow model
tf_pred_probs = model.predict(X_test)
tf_pred_classes = tf.argmax(tf_pred_probs, axis=1).numpy()

"""## 6. Evaluation dan Visualization"""

# List semua model prediksi
model_names = ['Random Forest', 'XGBoost', 'KNN', 'MLP Sklearn', 'MLP TensorFlow']
predictions = [rf_pred, xgb_pred, knn_pred, mlp_pred, tf_pred_classes]

# Confusion Matrix dan Classification Report
for name, pred in zip(model_names, predictions):
    print(f"\n=== {name} ===")
    print(classification_report(y_test, pred, target_names=label_encoder.classes_))

    cm = confusion_matrix(y_test, pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)
    disp.plot(cmap='Blues')
    plt.title(f'Confusion Matrix - {name}')
    plt.show()

"""## 7. Menentukan Best Model

### 7.1 Bandingkan Performa Model
"""

# Buat fungsi evaluasi
def evaluate_model(name, y_true, y_pred):
    return {
        'Model': name,
        'Accuracy': accuracy_score(y_true, y_pred),
        'Precision': precision_score(y_true, y_pred, average='macro'),
        'Recall': recall_score(y_true, y_pred, average='macro'),
        'F1 Score': f1_score(y_true, y_pred, average='macro')
    }

# Evaluasi semua model
results = []
results.append(evaluate_model('Random Forest', y_test, rf_pred))
results.append(evaluate_model('XGBoost', y_test, xgb_pred))
results.append(evaluate_model('KNN', y_test, knn_pred))
results.append(evaluate_model('MLP Sklearn', y_test, mlp_pred))
results.append(evaluate_model('MLP TensorFlow', y_test, tf_pred_classes))

# Tabel hasil
import pandas as pd
results_df = pd.DataFrame(results)
print(results_df)

# Visualisasi
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
results_melted = results_df.melt(id_vars="Model", var_name="Metric", value_name="Score")
sns.barplot(data=results_melted, x="Model", y="Score", hue="Metric")
plt.title("Perbandingan Performa Model")
plt.xticks(rotation=15)
plt.legend(loc="lower right")
plt.tight_layout()
plt.show()

"""### 7.2 Diagnosa Overfitting / Underfitting"""

# Akurasi training vs testing (untuk Sklearn models)
print("Random Forest")
print("Train Accuracy:", accuracy_score(y_train, rf_clf.predict(X_train)))
print("Test Accuracy :", accuracy_score(y_test, rf_pred))

print("\nXGBoost")
print("Train Accuracy:", accuracy_score(y_train, xgb_clf.predict(X_train)))
print("Test Accuracy :", accuracy_score(y_test, xgb_pred))

print("\nKNN")
print("Train Accuracy:", accuracy_score(y_train, knn_clf.predict(X_train)))
print("Test Accuracy :", accuracy_score(y_test, knn_pred))

print("\nMLP Sklearn")
print("Train Accuracy:", accuracy_score(y_train, mlp_clf.predict(X_train)))
print("Test Accuracy :", accuracy_score(y_test, mlp_pred))

# Plot training history untuk TensorFlow
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('Akurasi MLP TensorFlow')
plt.xlabel('Epoch')
plt.ylabel('Akurasi')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Loss MLP TensorFlow')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

"""### 7.3 Hyperparameter Tuning"""

# Random Forest
rf_params = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5]
}
rf_grid = GridSearchCV(rf_clf, rf_params, cv=3, scoring='f1_macro', n_jobs=-1)
rf_grid.fit(X_train, y_train)
print("Best Random Forest Params:", rf_grid.best_params_)
print("Best RF Score:", rf_grid.best_score_)

# XGBoost
xgb_params = {
    'n_estimators': [100, 200],
    'max_depth': [3, 6],
    'learning_rate': [0.05, 0.1]
}
xgb_grid = GridSearchCV(xgb_clf, xgb_params, cv=3, scoring='f1_macro', n_jobs=-1)
xgb_grid.fit(X_train, y_train)
print("Best XGBoost Params:", xgb_grid.best_params_)
print("Best XGB Score:", xgb_grid.best_score_)

# KNN
knn_params = {
    'n_neighbors': [3, 5, 7],
    'weights': ['uniform', 'distance']
}
knn_grid = GridSearchCV(knn_clf, knn_params, cv=3, scoring='f1_macro', n_jobs=-1)
knn_grid.fit(X_train, y_train)
print("Best KNN Params:", knn_grid.best_params_)
print("Best KNN Score:", knn_grid.best_score_)

"""### 8. Konversi Model"""

# Simpan model terlebih dahulu
try:
    model_mlp_tf.save('model_tf_weather.h5') # Pastikan nama variabel model Keras Anda adalah model_mlp_tf
    print("--- TAHAP 1: Model Keras (.h5) berhasil disimpan.")
except Exception as e:
    print(f"--- TAHAP 1: ERROR saat menyimpan model .h5: {e}")


# Simpan scaler
try:
    print(f"--- TAHAP 2: Mencoba menyimpan scaler. Variabel scaler: {type(scaler)}") # Cek tipe variabel scaler
    with open('scaler.save', 'wb') as f_scaler:
        pickle.dump(scaler, f_scaler)
    print("--- TAHAP 2: Scaler (scaler.save) berhasil disimpan.")
except Exception as e:
    print(f"--- TAHAP 2: ERROR saat menyimpan scaler: {e}")


# Simpan label encoder
try:
    print(f"--- TAHAP 3: Mencoba menyimpan label_encoder. Variabel label_encoder: {type(label_encoder)}") # Cek tipe variabel label_encoder
    if label_encoder is not None: # Tambahkan pengecekan ini
        with open('label_encoder.save', 'wb') as f_encoder:
            pickle.dump(label_encoder, f_encoder)
        print("--- TAHAP 3: Label Encoder (label_encoder.save) berhasil disimpan.")
    else:
        print("--- TAHAP 3: Variabel label_encoder adalah None, tidak bisa disimpan.")
except Exception as e:
    print(f"--- TAHAP 3: ERROR saat menyimpan label encoder: {e}")




# Konversi ke .tflite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Simpan file .tflite
with open("model_tf_weather.tflite", "wb") as f:
    f.write(tflite_model)

print("âœ… Model berhasil dikonversi ke .tflite")




"""## 9. Inference"""

# Menggunakan indexing NumPy biasa karena X_test adalah numpy.ndarray
sample_hujan = X_test[y_test == label_encoder.transform(['hujan'])[0]][0]
sample_mendung = X_test[y_test == label_encoder.transform(['mendung'])[0]][0]
sample_cerah = X_test[y_test == label_encoder.transform(['cerah'])[0]][0]

samples = pd.DataFrame([sample_hujan, sample_mendung, sample_cerah])
samples.reset_index(drop=True, inplace=True)

# Prediksi
pred_probs = model.predict(samples)
pred_classes = tf.argmax(pred_probs, axis=1).numpy()
pred_labels = label_encoder.inverse_transform(pred_classes)

# Tampilkan hasil
print("Hasil Inference:")
for i, label in enumerate(['Hujan', 'Mendung', 'Cerah']):
    print(f"Data Sampel: {label}")
    print(f"Prediksi: {pred_labels[i]}")
    print(f"Probabilitas: {pred_probs[i]}")
    print("-" * 30)